{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_classifier.py\n",
    "# loads data from database\n",
    "# splits dataset into training and test sets\n",
    "# builds a text processing and machine learning pipeline\n",
    "# trains and tunes a model using grid search\n",
    "# outputs results on the test set\n",
    "# exports the final model as a pickle file\n",
    "# builds visuals for front end from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import pickle\n",
    "import buildvisuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(filename):\n",
    "    '''\n",
    "    load data from database into dataframe\n",
    "    identify target columns\n",
    "    test that there's 36 columns\n",
    "    return target columns and dataframe\n",
    "    '''\n",
    "    # load\n",
    "    conn = sqlite3.connect(filename)\n",
    "    df = pd.read_sql('SELECT * FROM MessCatRaw', con = conn)\n",
    "    # targets\n",
    "    t_cols = df.columns[2:-2].tolist()\n",
    "    # test\n",
    "    assert (len(t_cols) == 36)\n",
    "\n",
    "    return t_cols, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomTokenize(text):\n",
    "    '''\n",
    "    input text string\n",
    "    clean and lower case characters of string\n",
    "    tokenize text\n",
    "    lematize and remove stop words\n",
    "    add bigrams\n",
    "    return cleaned tokens and bigrams\n",
    "    '''\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # normalize\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(s)\n",
    "    # stemming\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    # add bi-grams\n",
    "    bigrams = [a + ' ' + b for a,b in list(nltk.bigrams(tokens))]\n",
    "    return tokens + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildFreqFilter(df):\n",
    "    '''\n",
    "    import datframe\n",
    "    tokenize messages\n",
    "    build counter of word frequencies\n",
    "    build filter dictionary\n",
    "    save filter dictionary\n",
    "    test that the vocab is over 30,000 tokens\n",
    "    return filter dictionary\n",
    "    '''\n",
    "    messages = df['message']\n",
    "    texts = messages.apply(CustomTokenize).tolist()\n",
    "    dwf = Counter(chain.from_iterable(texts))\n",
    "    dtf = {x: count for x, count in dwf.items() if count >= 5}\n",
    "    assert(len(dtf) > 30000)\n",
    "    return dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = BuildFreqFilter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(text, dtf=dtf):\n",
    "    '''\n",
    "    input text string and filter dictionary\n",
    "    custom tokenize\n",
    "    apply filter\n",
    "    return list of filtered tokens\n",
    "    '''\n",
    "    tokens = CustomTokenize(text)\n",
    "    return [t for t in tokens if t in dtf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(df, t_cols):\n",
    "    '''\n",
    "    input dataframe and targets\n",
    "    split off validation set\n",
    "    split off train and test data\n",
    "    assign targets and features for validation\n",
    "    assign targets and features for train and test\n",
    "    split train and test data\n",
    "    return datasets\n",
    "    '''\n",
    "    # split off validation set\n",
    "    dfv = df[df['val']==1].copy()\n",
    "    # independant train and test data \n",
    "    dft = df[df['val']==0].copy()\n",
    "\n",
    "    # assign features and targets for validation\n",
    "    Xval = dfv['message'].copy()\n",
    "    Yval = dfv[t_cols].copy()\n",
    "\n",
    "    # assign features and targets for train and test\n",
    "    X = dft['message'].copy()\n",
    "    Y = dft[t_cols].copy()\n",
    "\n",
    "    # test train split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    return dft, Xval,Yval,X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft, Xval,Yval,X_train,Y_train,X_test,Y_test = SplitData(df, t_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(X_train,Y_train):\n",
    "    '''\n",
    "    import train dataset\n",
    "    builds a text processing and machine learning pipeline\n",
    "    trains and tunes a model using grid search\n",
    "    returns optimized model\n",
    "    '''\n",
    "    # basic pipeline\n",
    "    over_samp = SMOTE()\n",
    "    under_samp = RandomUnderSampler()\n",
    "    logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    clf3 = make_pipeline(over_samp, under_samp, logreg)\n",
    "    pipe3 = Pipeline([('tfidf', TfidfVectorizer(tokenizer=PreProcess, \n",
    "                                                use_idf=True, \n",
    "                                                sublinear_tf=True, \n",
    "                                                ngram_range = (1,1))),\n",
    "                      ('skb',  SelectKBest(chi2, k=10000)),\n",
    "                      ('clf3', MultiOutputClassifier(clf3, n_jobs=-1))])\n",
    "    \n",
    "    # grid search definition\n",
    "    parameters = {'skb__k': [10000,20000,30000]}\n",
    "    model = GridSearchCV(pipe3, param_grid=parameters)\n",
    "\n",
    "    # train model\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BuildModel(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintClassReports(Y_predictions, Y_target):\n",
    "    '''\n",
    "    input classification predictions and targets\n",
    "    compute figures\n",
    "    print summary and estimate\n",
    "    '''\n",
    "    # classification reports\n",
    "    cols = Y_target.columns.tolist()\n",
    "    Y_targ = Y_target.values\n",
    "    print('------------------------------------------------------')\n",
    "    for i in range(36):\n",
    "        print(cols[i])\n",
    "        print(classification_report(Y_targ.T[i],Y_predictions.T[i],\n",
    "                                    zero_division=0))\n",
    "        print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "PrintClassReports(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveModel(model, dtf, filename):\n",
    "    '''\n",
    "    import model, filter dictionary, and filename\n",
    "    save for later\n",
    "    '''\n",
    "    # save model\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((model, dtf), f) \n",
    "\n",
    "    print('Model saved to : '+ filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Logregmod20.pkl'\n",
    "SaveModel(model, dtf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['prep'] = dft['message'].apply(PreProcess)\n",
    "buildvisuals.dft = dft\n",
    "buildvisuals.BuildFig(Y_train, 'train_disaster_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### python file version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run train_classifier ../data/DisasterResponse.db Logregmod21.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datasci)",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
